---
title: "Adventures in Covariance"
author: "Fernando Miguez"
date: "9/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(nlraa)
library(car)
library(brms)
library(nlme)
```

# Relationship between the intercept and slope 

Even in a simple linear regression there is a Variance-Covariance matrix for the two parameters of the model: the intercept and the slope. The model here is:

$$
y_i = \beta_0 + \beta_1 x_i + e_i
$$

It is natural for these parameters to be correlated becuase it is les likey to have extreme combinations for both of them. Here is a simple non-Bayesian example:

```{r}
## Simulate data
xx <- 1:20
yy <- 3 + 1.5 * xx + rnorm(length(xx))
dat <- data.frame(x = xx, y = yy)
## Visualize
ggplot(data = dat, aes(x = x, y = y)) + 
  geom_point()
## Fit a linear model
fit <- lm(y ~ x, data = dat)
## Beta coefficients
coef(fit)
## Variance-Covariance for beta
vcov(fit)
## Correlation matrix
cov2cor(vcov(fit))
## Visualize model fits
set.seed(1234)
fit.s <- simulate_lm(fit, nsim = 5, value = "data.frame")
ggplot(data = fit.s, aes(x = x, y = y)) + 
  geom_line(aes(y = sim.y, group = ii), color = "purple", alpha = 0.3) + 
  geom_point() 
## We can also visualize the negative correlation among the parameters
fit.b <- boot_lm(fit)
hist(fit.b, 1, ci = "perc")
hist(fit.b, 2, ci = "perc")
## Visualizing the correlation
plot(fit.b$t, xlab = "Intercept", ylab = "Slope")
```

The function **simulate_lm** uses internally the function **MASS::mvrnorm** which can simualte samples from a multivariate normal distribution. **boot_lm** does bootstrapping which involves simulating data similar to the observed one and re-fitting the model (by default 1000 times). This is a way of empirically deriving the distribution of the parameters.

## The brms way

How to fit a simple Bayesian linear regression using brms

```{r}
priors <- prior(normal(0, 5), class = "Intercept") + 
  prior(normal(0, 4), coef = "x")
## Fit model
fbrms <- brm(y ~ x, data = dat, prior = priors, refresh = 0)
## Plotting posterior distributions of parameters
plot(fbrms, "b_")
## Extract posterior samples
ps <- posterior_samples(fbrms)
## Simple plot
plot(ps[,1:2], xlab = "Intercept", ylab = "slope")
## Or using pairs
pairs(fbrms, "^b_")
## We can extract equivalent quantities
vcov(fbrms)
cov2cor(vcov(fbrms))
```

## How do we fit a model with varying intercepts and slopes in brms

```{r, cache = TRUE}
data(barley, package = "nlraa")
## Fit one regression for each year, with year as random
barley$NF2 <- barley$NF^2
br.lme <- lme(yield ~ NF + NF2, random = ~ NF + NF2 | year, data = barley)
## Fixed or population-level effects
fixef(br.lme)
## Variance-covariance for beta
vcov(br.lme)
cov2cor(vcov(br.lme))
## Bayesian way
prs <- prior(normal(100, 50), class = Intercept) + 
  prior(normal(50, 15), class = "b", coef = "NF") + 
  prior(normal(0, 5), class = "b", coef = "NF2") + 
  prior(student_t(2, 0, 150), class = "sd")
## Fit the multilevel model
br.brms <- brm(yield ~ NF + NF2 + (NF + NF2 | year), 
               data = barley, refresh = 0, iter = 10000,
               cores = 4,
               seed = 1234,
               prior = prs, control = list(adapt_delta = 0.95,
                                           max_treedepth = 15))
## I get some complains, but the model fit seems fine
plot(br.brms, "^b_")
pairs(br.brms, "^b_")
pp <- predict(br.brms)  
pp2 <- cbind(barley, pp)
## Plot
ggplot(data = pp2, aes(NF, yield)) + 
  geom_point() + 
  facet_wrap(~year) + 
  geom_line(aes(y = Estimate)) + 
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = "purple", alpha = 0.3)
```

# More on Covariances

Covariances arise any time we have two or more random variables. For a sinlge variable we have a mean and a variance.

$$
\mu = E(X) \\
\sigma^2 = E[(X  - E(X))^2]
$$
When we have two variables $X_1$ and $X_2$ then we have

$$
Cov = \left[ \begin{array}{ll}
             \sigma_1^2 & \sigma_{12} \\
             \sigma_{21} & \sigma_2^2
              \end{array} \right]
$$
In the simplest of linear regression models we have

$$
Cov(Y) = Cov(e) = \sigma^2 \times I_{n \times n}
$$

However, this Covariance could be modelled in much more complex ways when the data are more complex

```{r nlraa}
library(nlme)
library(nlraa)
library(ggplot2)
data(ChickWeight)

dim(ChickWeight)

fm1 <- lm(weight ~ Time, data = ChickWeight)

ggplot(data = ChickWeight, aes(Time, weight)) + 
  geom_point() + 
  ggtitle("Chick weight with time")

ggplot(data = ChickWeight, aes(Time, weight, color = Chick)) + 
  geom_point() + 
  geom_line(aes(y = fitted(fm1)), color = "black") + 
  theme(legend.position = "none")

## Fit a linear model where we fit 
## increasing variance and CAR1 correlation structure
fit4 <- gls(weight ~ Time, data = ChickWeight, 
            weights = varPower(),
            correlation = corCAR1(form = ~ Time | Chick))

v4 <- var_cov(fit4)
## Tip: you can visualize these matrices using
image(log(v4[,ncol(v4):1]))
## Take a closer look
v42 <- v4[1:36, 1:36]
image(log(v42[,ncol(v42):1]))

## Look at raw numbers
round(v4[1:12, 1:12])
## Look at ChickWeight
head(ChickWeight, 12)
## Look at the correlation matrix
round(cov2cor(v4[1:12, 1:12]), 2)
```

Some magic happened in when we fitted the model using **gls**. The **varPower** option assumes that the variance in the diagonal can be modeled in the following way:

$$
\sigma^2(v) = |v|^{2 t}
$$
```{r chickweight-gls}
fit4
```

The correlation was modeled using **corCAR1** (https://en.wikipedia.org/wiki/Autoregressive_model).

![SAS-cov](./SAS-covariances.png)





